{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization with Scikit-Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform **Bayesian Optimization** with Gaussian Processes in Parallel, utilizing various CPUs, to speed up the search.\n",
    "\n",
    "This is useful to reduce search times.\n",
    "\n",
    "https://scikit-optimize.github.io/stable/auto_examples/parallel-optimization.html#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from skopt import Optimizer # for the optimization\n",
    "from joblib import Parallel, delayed # for the parallelization\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in skopt.Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 120, name=\"n_estimators\"),\n",
    "    Integer(1, 5, name=\"max_depth\"),\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'),\n",
    "    Real(0.001, 0.999, prior='log-uniform', name=\"min_samples_split\"),\n",
    "    Categorical(['deviance', 'exponential'], name=\"loss\"),\n",
    "]\n",
    "\n",
    "# Scikit-optimize parameter grid is a list\n",
    "type(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the gradient boosting classifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We design a function to maximize the accuracy, of a GBM,\n",
    "# with cross-validation\n",
    "\n",
    "# the decorator allows our objective function to receive the parameters as\n",
    "# keyword arguments. This is a requirement for scikit-optimize.\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    gbm.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(\n",
    "        cross_val_score(\n",
    "            gbm, \n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=3,\n",
    "            n_jobs=-4,\n",
    "            scoring='accuracy')\n",
    "    )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Optimizer\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    dimensions = param_grid, # the hyperparameter space\n",
    "    base_estimator = \"GP\", # the surrogate\n",
    "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    random_state=0, \n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/santiago/anaconda3/envs/hyperparameter_optimization_for_machine_learning/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# we will use 4 CPUs (n_points)\n",
    "# if we loop 10 times using 4 end points, we perform 40 searches in total\n",
    "\n",
    "for i in range(10):\n",
    "    x = optimizer.ask(n_points=4)  # x is a list of n_points points\n",
    "    y = Parallel(n_jobs=4)(delayed(objective)(v) for v in x)  # evaluate points in parallel\n",
    "    optimizer.tell(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[68, 4, 0.007381238832487747, 0.08704800719052391, 'exponential'],\n",
       " [118, 2, 0.00010113718979245275, 0.05725990689319986, 'deviance'],\n",
       " [17, 3, 0.00022221129238269847, 0.17371729808265857, 'exponential'],\n",
       " [42, 4, 0.000960176974739521, 0.6793527084375192, 'exponential'],\n",
       " [38, 5, 0.053126979002083165, 0.06277193933628669, 'deviance'],\n",
       " [28, 4, 0.005445788189169609, 0.002258808366805843, 'deviance'],\n",
       " [56, 1, 0.0006780193306440557, 0.9274466173670369, 'exponential'],\n",
       " [42, 4, 0.08952334707464486, 0.20644568894340298, 'deviance'],\n",
       " [68, 1, 0.0010637763908757617, 0.0037524397848558923, 'deviance'],\n",
       " [48, 4, 0.0016815858162959685, 0.27886812907463643, 'deviance'],\n",
       " [63, 3, 0.016573826638511105, 0.02731983287242903, 'exponential'],\n",
       " [84, 5, 0.004221241398982209, 0.025088920346216202, 'exponential'],\n",
       " [103, 5, 0.015344947566442498, 0.999, 'exponential'],\n",
       " [120, 5, 0.08918658438831356, 0.999, 'exponential'],\n",
       " [74, 5, 0.05615454778606038, 0.999, 'exponential'],\n",
       " [120, 5, 0.031555061624740456, 0.999, 'exponential'],\n",
       " [10, 1, 0.07717423197337833, 0.001, 'exponential'],\n",
       " [10, 1, 0.07458029962842769, 0.001, 'exponential'],\n",
       " [17, 2, 0.0728798801275082, 0.001, 'exponential'],\n",
       " [24, 2, 0.07307560630045908, 0.001, 'exponential'],\n",
       " [120, 5, 0.07521890633681927, 0.001, 'deviance'],\n",
       " [120, 5, 0.07095340541742816, 0.001, 'deviance'],\n",
       " [120, 5, 0.07138559940863284, 0.0029151968785866927, 'deviance'],\n",
       " [90, 5, 0.1, 0.001, 'exponential'],\n",
       " [120, 5, 0.06622633186920465, 0.999, 'exponential'],\n",
       " [120, 1, 0.025580398827872188, 0.999, 'exponential'],\n",
       " [10, 1, 0.025626040708475857, 0.999, 'exponential'],\n",
       " [116, 5, 0.07121495742003589, 0.999, 'exponential'],\n",
       " [72, 1, 0.1, 0.06507884842409951, 'exponential'],\n",
       " [51, 2, 0.0848768037987019, 0.001, 'deviance'],\n",
       " [120, 1, 0.0055965913244677415, 0.001, 'exponential'],\n",
       " [88, 1, 0.011520013672123113, 0.001, 'exponential'],\n",
       " [59, 1, 0.042924930487493525, 0.0018709189921355133, 'exponential'],\n",
       " [70, 5, 0.028216693618319955, 0.999, 'deviance'],\n",
       " [78, 1, 0.027709982164713983, 0.999, 'exponential'],\n",
       " [12, 5, 0.1, 0.2609378728798426, 'deviance'],\n",
       " [58, 5, 0.06774976309372796, 0.999, 'exponential'],\n",
       " [120, 5, 0.014143308758261244, 0.999, 'exponential'],\n",
       " [120, 5, 0.016179849161364706, 0.001, 'deviance'],\n",
       " [100, 4, 0.04362031500023444, 0.001, 'exponential']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the evaluated hyperparamters\n",
    "\n",
    "optimizer.Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9171413381939697,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9296536796536796,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9321979190400244,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9397357028935976,\n",
       " -0.9120528594212804,\n",
       " -0.9296346927925875,\n",
       " -0.9522480443533076,\n",
       " -0.9497607655502392,\n",
       " -0.9472544998860788,\n",
       " -0.9195336826915774,\n",
       " -0.9170084301663248,\n",
       " -0.9246411483253588,\n",
       " -0.9296536796536796,\n",
       " -0.9322548796233007,\n",
       " -0.9221538695222905,\n",
       " -0.9221538695222905,\n",
       " -0.929691653375864,\n",
       " -0.9472165261638946,\n",
       " -0.9497607655502392,\n",
       " -0.6256360598465861,\n",
       " -0.9472165261638946,\n",
       " -0.9472165261638946,\n",
       " -0.9397357028935976,\n",
       " -0.8994645705172021,\n",
       " -0.9195336826915774,\n",
       " -0.9472544998860788,\n",
       " -0.9296536796536796,\n",
       " -0.9321409584567478,\n",
       " -0.9296536796536796,\n",
       " -0.9497607655502392,\n",
       " -0.9296346927925875,\n",
       " -0.9246411483253588,\n",
       " -0.9322169059011164]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the accuracy\n",
    "\n",
    "optimizer.yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "0            68          4           0.007381       0.087048  exponential   \n",
       "1           118          2           0.000101       0.057260     deviance   \n",
       "2            17          3           0.000222       0.173717  exponential   \n",
       "3            42          4           0.000960       0.679353  exponential   \n",
       "4            38          5           0.053127       0.062772     deviance   \n",
       "\n",
       "   accuracy  \n",
       "0 -0.917141  \n",
       "1 -0.625636  \n",
       "2 -0.625636  \n",
       "3 -0.625636  \n",
       "4 -0.929654  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all together in one dataframe, so we can investigate further\n",
    "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']\n",
    "\n",
    "tmp = pd.concat([\n",
    "    pd.DataFrame(optimizer.Xi),\n",
    "    pd.Series(optimizer.yi),\n",
    "], axis=1)\n",
    "\n",
    "tmp.columns = dim_names + ['accuracy']\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate convergence of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp['accuracy'].sort_values(ascending=False).reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off with parallelization, is that we will not optimize the search after each evaluation of f(x), instead after, in this case 4, evaluations of f(x). Thus, we may need to perform more evaluations to find the optima. But, because we do it in parallel, overall, we reduce wall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.089187</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.952248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>0.067750</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0.056155</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025580</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042925</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.065079</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.066226</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>116</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071215</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>0.084877</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.939736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.027320</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.939736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075219</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.932255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043620</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.932217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.206446</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.932198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027710</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.932141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>0.028217</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.260938</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.073076</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.924641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072880</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.924641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.070953</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.922154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071386</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.922154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077174</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074580</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.912053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.899465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.278868</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025626</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "13           120          5           0.089187       0.999000  exponential   \n",
       "36            58          5           0.067750       0.999000  exponential   \n",
       "14            74          5           0.056155       0.999000  exponential   \n",
       "25           120          1           0.025580       0.999000  exponential   \n",
       "15           120          5           0.031555       0.999000  exponential   \n",
       "32            59          1           0.042925       0.001871  exponential   \n",
       "28            72          1           0.100000       0.065079  exponential   \n",
       "24           120          5           0.066226       0.999000  exponential   \n",
       "27           116          5           0.071215       0.999000  exponential   \n",
       "29            51          2           0.084877       0.001000     deviance   \n",
       "10            63          3           0.016574       0.027320  exponential   \n",
       "20           120          5           0.075219       0.001000     deviance   \n",
       "39           100          4           0.043620       0.001000  exponential   \n",
       "7             42          4           0.089523       0.206446     deviance   \n",
       "34            78          1           0.027710       0.999000  exponential   \n",
       "23            90          5           0.100000       0.001000  exponential   \n",
       "33            70          5           0.028217       0.999000     deviance   \n",
       "35            12          5           0.100000       0.260938     deviance   \n",
       "19            24          2           0.073076       0.001000  exponential   \n",
       "4             38          5           0.053127       0.062772     deviance   \n",
       "37           120          5           0.014143       0.999000  exponential   \n",
       "12           103          5           0.015345       0.999000  exponential   \n",
       "38           120          5           0.016180       0.001000     deviance   \n",
       "18            17          2           0.072880       0.001000  exponential   \n",
       "21           120          5           0.070953       0.001000     deviance   \n",
       "22           120          5           0.071386       0.002915     deviance   \n",
       "16            10          1           0.077174       0.001000  exponential   \n",
       "31            88          1           0.011520       0.001000  exponential   \n",
       "0             68          4           0.007381       0.087048  exponential   \n",
       "17            10          1           0.074580       0.001000  exponential   \n",
       "11            84          5           0.004221       0.025089  exponential   \n",
       "30           120          1           0.005597       0.001000  exponential   \n",
       "9             48          4           0.001682       0.278868     deviance   \n",
       "26            10          1           0.025626       0.999000  exponential   \n",
       "5             28          4           0.005446       0.002259     deviance   \n",
       "3             42          4           0.000960       0.679353  exponential   \n",
       "2             17          3           0.000222       0.173717  exponential   \n",
       "1            118          2           0.000101       0.057260     deviance   \n",
       "8             68          1           0.001064       0.003752     deviance   \n",
       "6             56          1           0.000678       0.927447  exponential   \n",
       "\n",
       "    accuracy  \n",
       "13 -0.952248  \n",
       "36 -0.949761  \n",
       "14 -0.949761  \n",
       "25 -0.949761  \n",
       "15 -0.947254  \n",
       "32 -0.947254  \n",
       "28 -0.947217  \n",
       "24 -0.947217  \n",
       "27 -0.947217  \n",
       "29 -0.939736  \n",
       "10 -0.939736  \n",
       "20 -0.932255  \n",
       "39 -0.932217  \n",
       "7  -0.932198  \n",
       "34 -0.932141  \n",
       "23 -0.929692  \n",
       "33 -0.929654  \n",
       "35 -0.929654  \n",
       "19 -0.929654  \n",
       "4  -0.929654  \n",
       "37 -0.929635  \n",
       "12 -0.929635  \n",
       "38 -0.924641  \n",
       "18 -0.924641  \n",
       "21 -0.922154  \n",
       "22 -0.922154  \n",
       "16 -0.919534  \n",
       "31 -0.919534  \n",
       "0  -0.917141  \n",
       "17 -0.917008  \n",
       "11 -0.912053  \n",
       "30 -0.899465  \n",
       "9  -0.625636  \n",
       "26 -0.625636  \n",
       "5  -0.625636  \n",
       "3  -0.625636  \n",
       "2  -0.625636  \n",
       "1  -0.625636  \n",
       "8  -0.625636  \n",
       "6  -0.625636  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hyperparameter_optimization_for_machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f33e3a0ecdca2dfa9e05d6e5730c703bfe1984a713822542070e9abfcf88fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
